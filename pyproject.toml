[project]
name = "linearmem"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "accelerate>=1.12.0",
    "bitsandbytes>=0.49.1",
    "datasets>=4.5.0",
    "einops>=0.8.1",
    "flash-attn",
    "peft>=0.18.1",
    "torch>=2.10.0",
    "torchaudio>=2.10.0",
    "torchvision>=0.25.0",
    "transformers>=4.57.6",
    "trl>=0.27.1",
    "wandb>=0.24.0",
]

[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[tool.setuptools.package-dir]
# 告诉 Python：当你 import qwen3mem 时，去这个路径找代码
"qwen3mem" = "src/Memllm/transformers/qwen3mem"

[tool.setuptools.packages.find]
# 确保只包含你想要的包
where = ["src/Memllm/transformers"]
include = ["qwen3mem*"]

[tool.uv.sources]
flash-attn = { url = "https://github.com/mjun0812/flash-attention-prebuild-wheels/releases/download/v0.7.12/flash_attn-2.6.3+cu128torch2.10-cp313-cp313-linux_x86_64.whl" }
